## 使用的工具
- [[FFmpeg（命令行视频抽帧）]]
- [[miniconda+labelimg（处理数据集）]]
- [[wsl（可以用宿主机的gpu）]]
## 问题1
我现在打算用yolo模型来做巡线到十字路口根据数字选择分支，但我不大会，该怎么部署这个模型
## 问题2
我们怎么从yolov5s.pt 基础模型开始进行微调，可以从我给你提供的这些下载下来训练吗，用markdown输出

## 问题1
好的，当然。这是一个使用 Markdown 格式化后的版本，结构更清晰，便于阅读。

---

这是一个非常棒且非常典型的机器人综合项目，它结合了传统的计算机视觉（巡线）和深度学习（目标检测），是学习和实践的绝佳案例。

我们不建议自始至终都只用一个复杂的YOLO模型来完成所有事情（包括巡线和数字识别），因为这样效率很低，而且调试困难。

我们推荐一种更稳定、更高效的**混合策略**：

- **平时**：使用简单、快速的OpenCV颜色处理方法进行巡线。
    
- **遇到十字路口时**：**激活**YOLO模型，专门用来识别数字，并根据结果做出决策。
    

---

## 模型部署与开发分步指南

整个过程分为三大步：**1. 模型训练（在PC上）** -> **2. 模型转换（在PC上）** -> **3. 板端部署（在机器人上）**。

#### 第一步：数据准备与模型训练（在PC上完成）

您的首要任务是训练一个能认识数字的YOLO模型。

1. **收集数据**：
    
    - 在您要比赛或运行的场地上，用机器人的摄像头**拍摄大量包含数字（例如 "1", "2", "3"）的图片**。
        
    - **关键**：图片要包含各种角度、光照条件、距离远近、甚至轻微模糊的情况，数据越多，模型越鲁棒。至少为每个数字类别准备100-200张图片。
        
2. **标注数据**：
    
    - 使用标注工具（如 `labelimg` 或在线的 `CVAT`、`Roboflow`）为您的图片打上标签。
        
    - 操作很简单：在每张图片的数字周围画一个框（bounding box），并给它指定一个类别（例如，框住数字"1"，就给它打上"1"的标签）。
        
3. **选择并训练模型**：
    
    - **选择模型**：为了在边缘设备上流畅运行，请选择轻量级的YOLO模型，例如 **YOLOv5s** 或 **YOLOv8n**。
        
    - **开始训练**：使用您标注好的数据集，在带有GPU的电脑上开始训练。YOLOv5和YOLOv8都有非常成熟的开源代码库，您只需要按照它们的官方文档指引，配置好数据集路径，就可以一键开始训练。
        
    - **训练结果**：训练完成后，您会得到一个模型权重文件，通常是 `.pt` 格式。这个文件就是您定制的、能认识数字的AI模型。
        

#### 第二步：模型转换（在PC上完成）

这一步是将您在PC上训练好的 `.pt` 模型，转换成能在地平线BPU上高效运行的格式。

1. **准备工具**：下载并安装地平线官方的 **“天工开物”（Togetheo）AIoT DDK** 开发工具包。
    
2. **执行转换**：使用DDK中提供的模型转换工具，对您的 `.pt` 文件进行转换和**量化**。这个过程会生成一个后缀为 `.bin` 或 `.hbm` 的文件。
    
    - 这个过程是整个部署流程中最具技术挑战性的一环，您需要**严格遵循地平线官方的模型转换文档**来操作。
        

#### 第三步：板端部署（机器人上的代码）

这是项目的核心，我们需要编写一个**状态机（State Machine）**来控制机器人的行为。

1. **定义机器人的三种状态：**
    
    - `FOLLOWING_LINE` (正在巡线)
        
    - `AT_CROSSROADS` (到达路口，准备识别)
        
    - `TURNING` (正在转弯)
        
2. **编写代码逻辑** 在您的ROS2节点中（可以基于之前的`follower`节点修改），`image_callback`函数需要根据当前状态执行不同操作。
    
    Python
    
    ```
    # 这是一个简化的伪代码结构
    class AutonomousNavigator(Node):
        def __init__(self):
            # ... 此处进行各种初始化 ...
            super().__init__('autonomous_navigator')
            self.state = "FOLLOWING_LINE"
            # 在这里加载转换好的 .hbm 模型
            self.yolo_model = bpu_infer_lib.Infer(...)
            self.yolo_model.load_model("您的数字识别模型.hbm")
            # ... 其他初始化 ...
    
        def image_callback(self, msg):
            image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')
    
            if self.state == "FOLLOWING_LINE":
                # --- 执行OpenCV颜色巡线 ---
                # 使用您之前写的基于颜色阈值的巡线代码。
                # 它简单、快速、可靠。
    
                # --- 判断是否到达十字路口 ---
                # 判断条件可以是：
                # 1. 检测到的黄线突然消失。
                # 2. 检测到一条横向的黑线（路口停止线）。
                if self.is_at_crossroads(image):
                    self.get_logger().info("到达十字路口，切换到识别状态！")
                    self.stop_robot() # 让机器人停下或减速
                    self.state = "AT_CROSSROADS"
    
            elif self.state == "AT_CROSSROADS":
                # --- 在这个状态下，激活YOLO模型 ---
                # 1. 对图像进行预处理（缩放、填充等），使其符合模型输入要求。
                # 2. 调用 self.yolo_model.forward() 进行推理。
                # 3. 解析模型的输出，找到识别到的数字和它的置信度。
    
                recognized_number = self.run_yolo_inference(image)
    
                if recognized_number is not None:
                    self.get_logger().info(f"成功识别数字: {recognized_number}，准备转弯！")
                    self.target_direction = self.decide_direction(recognized_number)
                    self.state = "TURNING"
    
            elif self.state == "TURNING":
                # --- 根据识别结果执行转弯动作 ---
                # 例如：
                # if self.target_direction == "left":
                #     self.turn_left()
                # elif self.target_direction == "right":
                #     self.turn_right()
    
                # 在转弯动作完成后（可以通过延时或里程计判断）
                self.get_logger().info("转弯完成，恢复巡线状态！")
                self.state = "FOLLOWING_LINE"
    ```
    
3. **调试建议**
    
    - **分步测试**：先确保您的纯OpenCV巡线模块能稳定工作。再单独测试YOLO识别模块，看看它能否在您把数字卡片放到摄像头前时正确识别。
        
    - **打印日志**：在代码的关键位置（如状态切换、识别出数字时）使用 `self.get_logger().info()` 打印日志，方便您实时了解机器人当前的状态。
        
    - **可视化**：使用 `rqt_image_view` 查看机器人摄像头传回的图像，以及您处理后（画上检测框）的图像，这对于调试至关重要。
        

这个方案将一个复杂问题分解成了多个简单、可控的子任务，是实现您目标的稳健路径。祝您项目顺利！

## 问题2
好的，我们来详细梳理一下如何从零开始，使用 `yolov5s.pt` 基础模型进行微调，来训练一个能识别您自定义数字的模型。

首先，回答您最关键的问题：

### 一个至关重要的前提：`.bin` 文件无法用于训练！

您从地平线模型库下载的那些 `.bin` 文件，是**不能**用来进行训练或微调的。

- **`.pt` 文件（您需要自己准备的）**
    
    - 这是PyTorch框架保存的模型**权重文件**。您可以把它想象成模型的“源代码”或“配方”。它包含了模型的所有参数，这些参数可以在训练过程中被不断学习和优化。
        
    - **这是我们进行“微调训练”的起点。**
        
- **`.bin` / `.hbm` 文件（您下载的/最终生成的）**
    
    - 这是经过地平线“天工开物”工具链**编译、量化和优化**后的**可执行二进制文件**。您可以把它想象成一个编译好的 `.exe` 程序。
        
    - 它的目的是在地平线BPU上高效运行，其内部结构已经固化，**无法再进行任何训练或修改**。
        

所以，我们的目标是：**获取官方的 `yolov5s.pt` -> 用您的数据训练它 -> 得到您自己的 `digits.pt` -> 再把它转换为 `.bin` 文件。**

---

## YOLOv5s 微调详细步骤 (在PC上完成)

整个微调过程都在带有NVIDIA GPU的电脑上进行，而不是在RDK X5开发板上。

### 第一步：搭建训练环境 (在PC上)

1. **克隆YOLOv5官方代码库** 打开终端，运行git命令，将YOLOv5的完整项目代码下载到您的电脑。
    
    Bash
    
    ```
    git clone https://github.com/ultralytics/yolov5.git
    ```
    
2. **进入项目目录并安装依赖** 这个项目依赖PyTorch, OpenCV等库。`requirements.txt`文件已经列出了所有需要的库。
    
    Bash
    
    ```
    cd yolov5
    pip install -r requirements.txt
    ```
    
3. **下载`yolov5s.pt`预训练权重** 这是我们微调的“基础”。它已经在一个大型数据集（COCO）上进行了预训练，具备了很好的特征提取能力，我们只需要在此基础上教它认识数字即可。 您可以直接从YOLOv5的GitHub Releases页面下载，或者使用命令：
    
    Bash
    
    ```
    wget https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt
    ```
    

### 第二步：准备自定义数据集

这是最耗时但也是最重要的一步。

1. **创建文件夹结构** 在 `yolov5` 目录下，创建一个 `dataset` 文件夹，并组织成如下结构：
    
    ```
    yolov5/
    ├── dataset/
    │   ├── images/
    │   │   ├── train/  <-- 存放训练图片
    │   │   └── val/    <-- 存放验证图片
    │   └── labels/
    │       ├── train/  <-- 存放训练标签
    │       └── val/    <-- 存放验证标签
    └── ... (其他yolov5文件)
    ```
    
2. **收集并划分图片** 将您拍摄的所有数字图片，按大约8:2的比例，分别放入 `images/train` 和 `images/val` 文件夹。
    
3. **标注图片**
    
    - 使用 `labelimg` 等标注工具，为 `images` 文件夹下的每一张图片进行标注。（弄一个[Anaconda | ZJU Mirror](https://mirrors.zju.edu.cn/docs/anaconda/)）
        
    - 对于每张图片，在数字周围画框，并指定类别。例如，数字“1”是第0类，数字“2”是第1类，数字“3”是第2类。
        
    - 标注完成后，`labelimg` 会在 `labels` 文件夹下生成与图片同名的 `.txt` 文件。例如 `pic1.jpg` 会对应生成 `pic1.txt`。
        
4. **[[创建数据集配置文件 (`.yaml`)]]** 在 `yolov5/data` 目录下，创建一个新的文件，例如 `digits.yaml`，内容如下：
    
    YAML
    
    ```
    # 训练集和验证集的图片路径
    train: ../dataset/images/train
    val: ../dataset/images/val
    
    # 类别数量
    nc: 3  
    
    # 类别名称 (顺序必须和标注时一致！)
    names: ['1', '2', '3']
    ```
    

### 第三步：开始微调训练

一切准备就绪后，在 `yolov5` 的根目录下，打开终端，运行以下命令开始训练：

Bash

```
python train.py --img 640 --batch 16 --epochs 100 --data data/digits.yaml --weights yolov5s.pt --name digits_run1
```

**[[模型训练参数解释：]]**

- `--img 640`: 训练时输入的图片大小，保持640以匹配地平线模型的要求。
    
- `--batch 16`: 批处理大小，根据您GPU显存大小调整，如果显存小，就调小这个值（如8或4）。
    
- `--epochs 100`: 训练轮次，表示要将整个数据集完整训练100遍。您可以先从较小的轮次开始尝试。
    
- `--data data/digits.yaml`: 指向我们刚刚创建的数据集配置文件。
    
- `--weights yolov5s.pt`: **这是微调的关键！** 它告诉程序从 `yolov5s.pt` 的权重开始训练，而不是从零开始。
    
- `--name digits_run`: 为本次训练任务命名，训练结果会保存在 `runs/train/digits_run` 文件夹下。
    

### 第四步：获取训练结果

训练过程会在终端显示进度条和各项指标。训练完成后：

- 找到 `yolov5/runs/train/digits_run/weights/` 目录。
    
- 里面会有一个 `best.pt` 和 `last.pt` 文件。`best.pt` 是训练过程中验证集上表现最好的模型。
    

**这个 `best.pt` 文件，就是您最终得到的、认识您自定义数字的专属模型。**

接下来，您就可以拿着这个 `best.t` 文件，去执行我们之前讨论的**“第二步：模型转换”**，用“天工开物”DDK将其转换为可以在RDK X5上运行的 `.bin` 文件了。


### 模型转换

#### 第一阶段：环境准备与工具安装

这是所有工作的基础，请务必确保环境配置正确。

**1. 确认您的PC环境：**

- **操作系统**: 强烈建议使用 **Ubuntu 18.04 或 20.04 LTS (64-bit)**。虽然其他系统可能也能通过 Docker 等方式运行，但官方原生支持和测试最充分的是 Ubuntu。
    
- **Python 环境**: 建议使用 `virtualenv` 或 `conda` 创建一个独立的 Python 虚拟环境（例如 Python 3.8），以避免与系统或其他项目的库产生冲突。
    

**2. 下载“天工开物” AIoT DDK：**

- **访问官方渠道**: 前往**地平线开发者社区**或**地平线官方支持页面**。
    
- **寻找DDK**: 找到与您的目标芯片（如 旭日® X3, 征程® 5 等）相对应的 “天工开物” AIoT DDK 版本。DDK 通常会打包为一个 `.tar.gz` 文件。
    
    - **搜索关键词**：`"地平线 征程5 DDK 下载"` 或 `"地平线 旭日X3M DDK 下载"`。
        
    - **注意**：下载时通常需要注册并登录地平线开发者账号。
        

**3. 安装 DDK：**

- **解压文件**: 将下载的 DDK压缩包解压到您的工作目录。
    
    Bash
    
    ```
    # 示例命令，请替换成你下载的实际文件名
    tar -xvf HBT_DDK_V_x.x.x_xxxxxx.tar.gz
    ```
    
- **运行安装脚本**: 解压后，通常会有一个 `install.sh` 或类似的安装脚本。进入解压后的目录，并以 **root 权限**运行它。
    
    Bash
    
    ```
    cd ddk_directory # 进入解压后的 DDK 目录
    sudo ./install.sh
    ```
    
    该脚本会将 DDK 的核心组件（如编译器、库文件等）安装到系统的指定位置（通常是 `/opt/hobot/`）。同时，它可能会检查并提示您安装一些系统依赖，如 `build-essential`, `python3-dev` 等。
    

**4. 配置 DDK 运行环境：**

- **激活环境**: DDK 提供了一个环境配置脚本，每次您需要使用 DDK 工具时，都需要先 `source` 这个脚本来设置必要的环境变量（如 `PATH`, `LD_LIBRARY_PATH` 等）。
    
    Bash
    
    ```
    # 示例路径，请根据你的实际安装路径进行调整
    source /opt/hobot/ddk/ddk_x.x.x/set_env.sh
    ```
    
    为了方便，您可以将此命令添加到您的 `.bashrc` 或 `.zshrc` 文件中，这样每次打开新的终端时就会自动配置好环境。
    
- **安装 Python 工具包**: DDK 的模型转换工具链是基于 Python 的。进入 DDK 的 `ddk/samples/ai_toolchain/model_conversion` 目录，通常会有一个 `requirements.txt` 文件。使用 `pip` 安装所有依赖。
    
    Bash
    
    ```
    # 激活你的 Python 虚拟环境
    source /path/to/your/venv/bin/activate
    
    # 进入 DDK 的 Python 工具目录
    cd /opt/hobot/ddk/ddk_x.x.x/samples/ai_toolchain/model_conversion
    
    # 安装依赖
    pip install -r requirements.txt
    ```
    

#### 第二阶段：模型准备

在执行转换前，您的 `.pt` 模型需要做一些预处理。

**1. 导出为 ONNX 格式：**

地平线的工具链不直接支持 `.pt` 文件，而是以 **ONNX (Open Neural Network Exchange)** 作为标准的中间格式。因此，您需要先将 PyTorch 的 `.pt` 模型导出为 `.onnx`。

- **编写导出脚本**: 创建一个 Python 脚本（例如 `export_onnx.py`）。
    
- **核心代码**:
    
    Python
    
    ```
    import torch
    
    # 1. 加载你的 PyTorch 模型
    # 确保模型处于评估模式 (evaluation mode)
    model = YourModelClass(*args, **kwargs)
    model.load_state_dict(torch.load("your_model.pt"))
    model.eval()
    
    # 2. 创建一个虚拟输入张量 (dummy input)
    # 输入的尺寸和数据类型必须与你模型训练时一致
    # batch_size 通常设为 1
    dummy_input = torch.randn(1, 3, 224, 224) # 示例：(batch, channels, height, width)
    
    # 3. 设置 ONNX 导出参数
    input_names = ["input_tensor"]   # 输入节点的名称
    output_names = ["output_tensor"] # 输出节点的名称
    
    # 4. 执行导出
    torch.onnx.export(
        model,
        dummy_input,
        "your_model.onnx",
        verbose=False,
        input_names=input_names,
        output_names=output_names,
        opset_version=11,  # 建议使用 opset_version 10, 11 或 12，具体参考地平线文档
        dynamic_axes=None  # 除非需要动态尺寸，否则建议使用静态尺寸
    )
    
    print("ONNX model has been exported successfully!")
    ```
    
- **注意事项**:
    
    - **`opset_version`**: 非常重要。请查阅您所用 DDK 版本文档中推荐的 ONNX Opset 版本。
        
    - **动态输入 (Dynamic Axes)**: 如果您的模型需要支持变化的输入尺寸（例如，不同分辨率的图片），需要配置 `dynamic_axes` 参数。但这会增加转换的复杂性，如果非必须，建议先使用固定尺寸。
        
    - **不支持的算子**: 确保您的模型中没有使用 ONNX 或地平线 BPU 不支持的算子。如果存在，您需要修改模型结构，用等效的支持算子来替代。
        

**2. 准备量化用的校准数据 (Calibration Data)：**

量化的目的是找到一个最佳的映射关系，将 FP32 的数值范围映射到 INT8。为了找到这个映射，需要提供一小部分有代表性的真实数据，这个过程称为“校准”。

- **数据选择**: 从您的验证集或测试集中，挑选 **50-200 张**具有多样性的图片。这些图片应该能反映模型在真实场景中遇到的各种情况（不同光照、角度、背景等）。
    
- **数据预处理**: 这些校准图片**必须**经过和模型训练时**完全相同**的预处理流程（如 resize, normalize, to-tensor 等）。
    
- **数据格式**: 将预处理后的数据保存为 `.npy` (Numpy) 格式，或者准备一个可以逐个加载数据的文件夹。地平线的工具链会读取这些数据来分析模型中间层的激活值分布。
    

---

#### 第三阶段：执行模型转换与量化

这是技术挑战最高的一步，核心是调用 DDK 提供的 `hb_mapper` 工具。

**1. 编写 YAML 配置文件：**

`hb_mapper` 的所有行为都是通过一个 `.yaml` 配置文件来控制的。这是整个流程的“指挥中心”。

- **创建一个 YAML 文件**: 例如 `01_check_and_convert.yaml`。
    
- **配置内容详解**:
    
    YAML
    
    ```
    # 模型和输入信息
    model_parameters:
      # --- Part 1: 模型基本信息 ---
      onnx_model: "path/to/your/your_model.onnx" # 指定你的 ONNX 模型路径
      model_name: "your_model_name" # 给你的模型起个名字
      output_model_file_prefix: "your_model_name_bpu" # 指定输出的 .bin 文件名前缀
    
    # 输入信息配置
    input_parameters:
      # --- Part 2: 输入节点信息 ---
      input_name: "input_tensor" # 必须与 ONNX 导出时的 input_names 一致
      input_shape: "1,3,224,224" # 格式: N,C,H,W
      input_type: "rgb" # 输入数据的格式 (如 rgb, bgr, gray)
      input_layout: "NCHW" # 输入数据的布局 (PyTorch 默认为 NCHW)
      norm_type: "data_mean_and_scale" # 归一化类型，非常重要！
    
    # 校准和量化配置
    calibration_parameters:
      # --- Part 3: 量化校准配置 ---
      cal_data_dir: "path/to/your/calibration_data_folder" # 校准数据集的路径
      # 如果校准数据是经过预处理的 .npy 文件，就用 cal_data_dir
      # 如果是原始图片，需要在这里配置预处理步骤，但更推荐提前处理好
      algorithm: "kl" # 量化算法，'kl' (Kullback-Leibler divergence) 是常用选项
      quant_type: "int8" # 目标量化类型
    
    # 编译配置
    compiler_parameters:
      # --- Part 4: 编译器配置 ---
      core_num: 1 # 使用的 BPU 核心数
      optimize_level: "O3" # 优化等级
      march: "bernoulli2" # 目标 BPU 架构！必须正确选择
                         # 例如：旭日X3 是 "bernoulli2" (bpu_b2), 征程5 是 "bayes" (bpu_b3)
                         # 请务必查阅 DDK 文档确认您的芯片对应的 march 值
    ```
    

**2. 执行转换命令：**

一切准备就绪后，在已经 `source` 了 DDK 环境变量的终端中，执行 `hb_mapper` 命令。

- **进入工作目录**:
    
    Bash
    
    ```
    # 进入 DDK 的 ai_toolchain 示例目录
    cd /opt/hobot/ddk/ddk_x.x.x/samples/ai_toolchain/model_conversion
    ```
    
- **执行命令**:
    
    Bash
    
    ```
    # 确保你的 python 虚拟环境已激活
    python3 horizon_model_convert_service.py --config_file /path/to/your/01_check_and_convert.yaml
    ```
    
    或者直接调用 `hb_mapper makertbin`：
    
    Bash
    
    ```
    hb_mapper makertbin --config /path/to/your/01_check_and_convert.yaml \
                        --model-type onnx
    ```
    

**3. 分析输出与结果：**

- **日志**: 转换过程会产生详细的日志，打印在终端上。**请务必仔细阅读**，特别是 `[ERROR]` 和 `[WARNING]` 信息。
    
    - **常见错误**: 算子不支持、输入尺寸不匹配、`march` 类型错误、校准数据读取失败等。
        
- **生成的文件**: 如果转换成功，会在 `output_model_file_prefix` 指定的目录下生成多个文件，其中最重要的就是：
    
    - `your_model_name_bpu.bin`: **这就是最终的目标文件！** 它可以直接被部署到地平线板端。
        
    - `your_model_name_bpu.html`: 一个详细的模型可视化报告，可以在浏览器中打开。它会展示模型的每一层、算子是否在 BPU 上运行、以及每一层的性能预估等，是进行性能分析和问题排查的利器。
        
    - 其他日志和中间文件。
        

### 总结与关键要点

1. **文档是生命线**: 地平线的 DDK 和芯片架构更新很快，本教程提供的是一个通用的流程框架。**最终的技术细节（如 `march` 类型、支持的 Opset 版本、YAML 配置项等）必须以您手中 DDK 版本附带的官方文档为准！**
    
2. **环境一致性**: 确保 PC 端的开发环境（特别是 DDK 版本）与您目标板端系统（ যেমন，TogetherROS）的版本相匹配。
    
3. **ONNX 是桥梁**: `.pt` -> `.onnx` -> `.bin` 这个流程是固定的。导出高质量的 ONNX 文件是成功的一半。特别注意 **opset version** 和 **静态/动态尺寸**。
    
4. **量化校准数据是关键**: 校准数据的质量直接影响最终 `.bin` 模型的精度。数据必须经过与训练时完全相同的预处理。
    
5. **YAML 配置要细心**: `march`、`input_name`、`input_shape`、`norm_type` 等参数一旦写错，就会导致转换失败或模型在板端运行结果不正确。
    
6. **善用可视化报告**: 转换后生成的 `.html` 报告是调试的法宝。当遇到性能不达标或精度下降问题时，首先检查这个报告，看看是哪些层没有在 BPU 上运行（会回退到 CPU 执行），或者量化误差较大。
    

遵循以上详细步骤，您将能更顺利地完成从 `.pt` 到 `.bin` 的模型转换，为最终在地平线平台上的高效部署打下坚实的基础。

# 虚拟环境
#### ### 方案一：使用虚拟环境（强烈推荐）

这是最标准、最安全的做法。它会为您的项目创建一个独立的“沙盒”，这样项目所需的包装就不会与系统级的包装互相干扰。

1. **安装venv工具**（如果系统没有的话）：
    
    Bash
    
    ```
    sudo apt-get update
    sudo apt-get install python3-venv
    ```
    
2. **进入您的项目目录，并创建一个虚拟环境**。我们把它命名为 `yolov5_env`:
    
    Bash
    
    ```
    cd ~/yolov5 
    python3 -m venv yolov5_env
    ```
    
3. **激活该环境**:
    
    Bash
    
    ```
    source yolov5_env/bin/activate
    ```
    
    激活后，您会看到命令提示符的开头出现了 `(yolov5_env)` 字样。
    
4. **现在，再次运行安装命令**。这次安装的所有包都会被干净地安装在这个沙盒里：
    
    Bash
    
    ```
    pip install -r requirements.txt
    ```